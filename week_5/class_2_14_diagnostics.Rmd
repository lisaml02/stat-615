---
title: "class_2_14_diagnostics"
author: "lisa liubovich"
date: "2024-02-14"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Assessment tools: scatterplots and residual plots

To make fits versus residuals plot, get the fitted values and the residuals with `augment()`. Make a scatterplot of .`fitted` vs the `.resid` variables

```{r}
a_hibbs <- augment(lmhibbs)
ggplot(a_hibbs, aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0)
```

# Dataset 1: Gold Standard

## Dataset 1: Scatterplot

```{r}
library(tidyverse)
library(broom)
x <- runif(100, -3, 3)
y <- x + rnorm(100)
df <- data.frame(x = x, y = y)
```

```{r}
ggplot(data = df, mapping = aes(x = x, y = y)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)
```

look at the data in strips: does the horizontal spread look about equal? aka does it LOOK linear? for equal variance: does the vertical spread look about equal? In this case, yes to all of that.

## Dataset 1: Residual Plot

```{r}
lmout <- lm(y ~ x, data = df)
aout <- augment(lmout)
ggplot(data = aout, mapping = aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0)
```

are the points distributed equally? are they concentrated around the same variability? Is one section of the points' variance the same as another? Yes! that's what makes it a good plot

## Dataset 1: Summary

-   Means are straight lines

-   Residuals seem to be centered at 0 for all x�

-   Variance looks equal for all x�

-   Everything looks perfect

### exercise:

Describe any issues with this plot.

![](https://dcgerard.github.io/stat_415_615/03_diagnostics_files/figure-html/unnamed-chunk-4-1.png)

it's okay that there are gaps in the x's. What we should look at is whether or not the mean of the points follows the same linear trend from left to right (here it's pretty good). What about the vertical spread? that also looks pretty good. So this is another example of a perfect linear model dataset. We don't make assumptions about the distribution of the x's, we make assumptions about the linear model and whether or not the vertical spread shows equal variance.

# Dataset 2: Curved Monotone Relationship, Equal Variances

## Dataset 2: Scatterplot

```{r}
x <- runif(100, 0, 6)
x <- x - min(x) + 0.5
y <- log(x) * 20 + rnorm(100, sd = 4)
df <- data.frame(x = x, y = y)
```

```{r}
ggplot(data = df, mapping = aes(x = x, y = y)) +
  geom_point() +
  geom_smooth(method = lm, se = FALSE, formula = y~x)
```

What's wrong with this plot? First, the distribution is clearly curved. The vertical spread looks like its about the same.

So the constant variance assumption looks good here, but the curved relationships violates the linearity assumption.

Also, it is monotone. what does that mean? It means either always increasing or always decreasing.

How do we fix the linearity? Transform x.

## Dataset 2: Residual Plot

```{r}
lmout <- lm(y ~ x)
aout <- augment(lmout)
ggplot(data = aout, mapping = aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0)
```

Curved, no longer monotonous.

## Dataset 2: Summary

-   Curved (but always increasing *or* always decreasing) relationship between x and y .

-   Variance looks equal for all x

-   Residual plot has a parabolic shape.

-   These indicate a log transformation of x could help.

-   Why not log(y)? Because taking transforming y can change the variance, and we already have constant variance, so we do not want to mess with that.

    -   Recall, random variation occurs in the y direction, not the x direction.

## Dataset 2: Transformed x Scatterplot

```{r}
df <- mutate(df, x_log = log(x))
ggplot(data = df, mapping = aes(x = x_log, y = y)) +
  geom_point() +
  geom_smooth(method = lm, se = FALSE)
```

## Dataset 2: Transformed x Residual Plot

```{r}
lmout <- lm(y ~ x_log, data = df)
aout <- augment(lmout)
ggplot(data = aout, mapping = aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0)
```

doesn't matter that most of the data is on the right, the variance is relatively constant

# Dataset 3: Curved Non-monotone Relationship, Equal Variances

## Dataset 3: Scatterplot

```{r}
x <- runif(100, -3, 3)
y <- -x^2 + rnorm(100)
df <- data.frame(x = x, y = y)
```

```{r}
ggplot(data = df, mapping = aes(x = x, y = y)) +
  geom_point() +
  geom_smooth(method = lm, se = FALSE)
```

non monotone and curved with constant variance

you can't fix nonmonotonicity with transformation, so the fix is to fit a quadratic curve

## Dataset 3: Residual Plot

```{r}
lmout <- lm(y ~ x, data = df)
aout <- augment(lmout)
ggplot(data = aout, mapping = aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0)
```

## Dataset 3: Fitting E[Yi]=β0+β1Xi+β2X2i

```{r}
df <- mutate(df, x2 = x^2) ## create x^2 first
quad_lm <- lm(y ~ x + x2, data = df) ## lm of x^2 + x
aout <- augment(quad_lm)
ggplot(data = aout, mapping = aes(x = x, y = y)) +
  geom_point() +
  geom_line(mapping = aes(x = x, y = .fitted), col = "blue", linewidth = 1)
```

## Dataset 3: Solution 1 Residuals

```{r}
ggplot(data = aout, mapping = aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0)
```

# Dataset 4: Curved Monotone Relationship, Variance Increases with Y

## Dataset 4: Scatterplot

```{r}
x <- runif(100, 0, 2)
y <- exp(x + rnorm(100, sd = 1/2))
df <- data.frame(x = x, y = y)
```

```{r}
ggplot(data = df, mapping = aes(x = x, y = y)) +
  geom_point() +
  geom_smooth(method = lm, se = FALSE)
```

doesn't look like constant variance

## Dataset 4: Residual Plot

```{r}
lmout <- lm(y ~ x, data = df)
aout <- augment(lmout)
ggplot(data = aout, mapping = aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0)
```

funnel shape indicates that variance is not constant

## Dataset 4: Summary

-   Curved relationship between x and y

-   monotone

-   heteroskedasticity --\> non equal variance

fix: transofrm y, try log first

## Dataset 4: Solution

```{r}
df <- mutate(df, y_log = log(y))
ggplot(data = df, mapping = aes(x = x, y = y_log)) +
  geom_point() +
  geom_smooth(method = lm, se = FALSE)
```

## Dataset 4: Solution

```{r}
lmout <- lm(y_log ~ x, data = df)
aout <- augment(lmout)
ggplot(data = aout, mapping = aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0)
```

**Exercise**: What if you see something like this? Do you think logging y will help?

![](https://dcgerard.github.io/stat_415_615/03_diagnostics_files/figure-html/unnamed-chunk-26-1.png)

yes it will! look at how all the large fits are on the left.

# Dataset 5: Linear Relationship, Equal Variances, Skewed Distribution

## Dataset 5: Scatterplot

```{r}
x <- runif(200)
y <- 15 * x + rexp(200, 0.2)
df <- data.frame(x = x, y = y)
```

```{r}
ggplot(data = df, mapping = aes(x = x, y = y)) +
  geom_point() +
  geom_smooth(method = lm, se = FALSE)
```

error terms seem to be right skewed --\> more extreme observations of error terms on the left

this violates the normality assumption

## Dataset 5: Residual Plot

```{r}
lmout <- lm(y ~ x, data = df)
aout <- augment(lmout)
ggplot(data = aout, mapping = aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0)
```

## Dataset 5: Summary

-   Straight line relationship between x and y.

-   Variances about equal for all x

-   Skew for all x

-   Residual plots show skew.

## Dataset 5: Solution

-   Do nothing, but report skew (usually OK to do)

-   Be fancy, fit quantile regression:

Median(Yi)=β0+β1Xi

# Dataset 6: Linear Relationship, Unequal Variances

## Dataset 6: Scatterplot

```{r}
x <- runif(100)
y <- x + rnorm(100, sd = (x + 0.3)^2 / 2)
df <- data.frame(x = x, y = y)
```

```{r}
ggplot(data = df, mapping = aes(x = x, y = y)) +
  geom_point() +
  geom_smooth(method = lm, se = FALSE)
```

variances look unequal

## Dataset 6: Residual Plot

```{r}
lmout <- lm(y ~ x, data = df)
aout <- augment(lmout)
ggplot(data = aout, mapping = aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0)
```

funnel shape -\> unequal variance

You can try logging both x and y, sometimes that works. But that won't work here. Be careful about negative values.

## Dataset 6: Solution

```{r}
df <- mutate(df, y_log = log(y + 0.5), # to address negative and 0 values
             x_log = log(x))
```

```{r}
ggplot(data = df, mapping = aes(x = x_log, y = y_log)) +
  geom_point()
```

linearity looks fine, but we have heteroskedasticity.

-   The book will suggest weighted least squares (with weights inverse to the variance). This is a little old fashion

-   fix: the modern solution is to use **sandwich** estimates of the standard errors

```{r}
library(lmtest)
library(sandwich)
# vcov. tells the software how to estimate the standard errors
# vcovHC is a function to estimate sandwhich standard errors (Heteroskedastic Consistent)
cout <- coeftest(x = lmout, vcov. = vcovHC)
tidy(cout, conf.int = TRUE)
```

Compare to old standard errors

```{r}
tidy(lmout, conf.int = TRUE)
```

# Dataset 7: Outlying observations

```{r}
x <- runif(100, -3, 3)
y <- x + rnorm(100)
x[[100]] <- 2.6
y[[100]] <- 10
df <- data.frame(x = x, y = y)
```

```{r}
ggplot(data = df, mapping = aes(x = x, y = y)) +
  geom_point() +
  geom_smooth(method = lm, se = FALSE)
```

normality is violated because of the outlier

## Dataset 7: Residual Plot

```{r}
lmout <- lm(y ~ x, data = df)
aout <- augment(lmout)
ggplot(data = aout, mapping = aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0)
```

fix: fit to quantile regression

```{r}
library(quantreg)
rqout <- rq(y ~ x, data = df)
t_rq <- tidy(rqout, conf.int = TRUE)
t_rq
```

```{r}
tidy(lmout, conf.int = TRUE)
```

We have a large sample size, so the results do not change much.

# Dataset 8: Sequence plots of residuals to check independence

Sometimes, the residuals can exhibit **autocorrelation** where residuals close to each other in time (or a sequence) are more similar than those further away.

```{r}
x <- sort(runif(100, -3, 3))
epsilon <- rep(NA_real_, length.out = length(x))
epsilon[[1]] <- 0
rho <- 0.9
for (i in 2:length(epsilon)) {
  epsilon[[i]] <- 0.85 * epsilon[[i-1]] + rnorm(1)
}
y <- x + epsilon
df <- data.frame(x = x, y = y)
```

```{r}
ggplot(data = df, mapping = aes(x = x, y = y)) +
  geom_point() +
  geom_smooth(method = lm, se = FALSE)
```

```{r}
lmout <- lm(y ~ x, data = df)
aout <- augment(lmout)
aout <- mutate(aout, index = row_number())
ggplot(data = aout, mapping = aes(x = index, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0)
```

-   this is a specific issue with the **independence** assumption.

-   The OLS fit is still unbiased, but the standard errors will be too small (because we have less information than we think).

## Dataset 8: Solution

-   This is discussed in detail in Chapter 12 of KNNL.

-   You should first check if including omitted variables helps remove autocorrelation.

    -   E.g. plotting annual sales versus average price over time, if you are missing population size, then adjacent years probably have more similar errors.

-   You can try to estimate autocorrelation directly (Section 12.4 of KNNL).

-   You can also adjust the OLS standard errors using a similar approach to sandwich estimation. This is called the "Newey-West HAC" (Heteroskedasticity- and autocorrelation-consistent) estimate of the standard error.

```{r}
library(lmtest)
library(sandwich)
cout <- coeftest(x = lmout, vcov. = vcovHAC)
tidy(cout, conf.int = TRUE)
```

Compare to original standard errors

```{r}
tidy(lmout, conf.int = TRUE)
```

-   You should have large sample sizes for accurate inference (n≥00) for estimating the autocorrelation or adjusting the standard errors.

# Quantile-Quantile plots of residuals to check normality

Sample 0.25-quantile

![](https://dcgerard.github.io/stat_415_615/03_diagnostics_files/figure-html/unnamed-chunk-52-1.png)

Theoretical 0.25-quantile

![](https://dcgerard.github.io/stat_415_615/03_diagnostics_files/figure-html/unnamed-chunk-53-1.png)

## QQ-plots in R

We can evaluate the normality assumption from the bread and peace example.

```{r}
hibbs <- read_csv("https://dcgerard.github.io/stat_415_615/data/hibbs.csv")
lmhibbs <- lm(vote ~ growth, data = hibbs)
```

Use `augment()` from the `{broom}` package to obtain the residuals. Map the residuals to the `sample` aesthetic, then use `geom_qq()`. Make sure you specify `sample = .resid`, *not* `x = .resid`.

```{r}
aout <- augment(lmhibbs)
ggplot(data = aout, mapping = aes(sample = .resid)) +
  geom_qq() +
  geom_qq_line()
```

-   There is a small deviation, but it's not too bad.

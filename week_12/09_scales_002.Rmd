---
title: "Scale and Multicollinearity"
author: "David Gerard"
date: '`r Sys.Date()`'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Earnings data from regression and other stories:

```{r}
library(tidyverse)
library(broom)
earnings <- read_csv("https://dcgerard.github.io/stat_415_615/data/earnings.csv")
earnings <- mutate(earnings, log_earn = log(earn))
earnings <- filter(earnings, is.finite(log_earn))
ggplot(data = earnings, mapping = aes(x = height, y = log_earn)) +
  geom_point()
```

```{r}
lm_earn <- lm(log_earn ~ height, data = earnings)
tidy(lm_earn)
```

Get an R^2
```{r}
glance(lm_earn)
```

About 6% of the variation in log-earnings can be explained by height.

```{r}
newdf <- data.frame(height = 70)
exp(predict(lm_earn, newdata = newdf, interval = "prediction"))
```


# Estate Example
```{r}
estate <- read_csv("https://dcgerard.github.io/stat_415_615/data/estate.csv")
estate <- mutate(estate, log_price = log(price))
estate <- select(estate, log_price, bed, bath)
```

```{r}
lm_estate <- lm(log_price ~ bath + bed, data = estate)
anova(lm_estate)
```

SSR(X2) = 53.7
SSR(X1|X2) = 0.4
SSE(X1, X2) = 43

SSTO = SSR(X2) + SSR(X1|X2) + SSE(X1, X2)
     = 53.7 + 0.4 + 43
     = 97.1
     
R^2_{Y2} = SSR(X2) / SSTO
         = 53.7 / 97.1
         = 0.553
         
R^2_{Y1|2} = SSR(X1|X2) / SSE(X2)
           = 0.4 / (0.4 + 43)
           = 0.1%
           
```{r}
glance(lm_estate)
```

R^2 = 55.7%
    = (SSR(X2) + SSR(X2|X1)) / SSTO
    = SSR(X1, X2) / SSTO
    = (53.7 + 0.4) / 97.1


Row 1:
Reduced: $Y_i = \beta_0 + \epsilon_i$
Full: $Y_i = \beta_0 + \beta_2X_{i2} + \epsilon_i$

Row 2: 
Reduced: $Y_i = \beta_0 + \beta_2X_{i2} + \epsilon_i$
Full: $Y_i = \beta_0 + \beta_1X_{i1} + \beta_2X_{i2} + \epsilon_i$


Overall F-test
```{r}
glance(lm_estate)$p.value
```


# Choice of Scale

Let's create height on different scales:
```{r}
earnings <- mutate(earnings,
                   height_mm = height * 25.4,
                   height_in = height,
                   height_mi = height / 63360)
```

```{r}
lm_mm <- lm(log_earn ~ height_mm, data = earnings)
lm_in <- lm(log_earn ~ height_in, data = earnings)
lm_mi <- lm(log_earn ~ height_mi, data = earnings)
```

```{r}
tidy(lm_mm)
```

```{r}
tidy(lm_in)
```

```{r}
tidy(lm_mi)
```


```{r}
ggplot(earnings, aes(x = height_mi, y = log_earn)) +
  geom_point()
```



```{r}
sd(earnings$height_mm)
earnings <- mutate(earnings, height_cm = height_mm / 10)
sd(earnings$height_cm)
```


```{r}
earnings <- mutate(earnings, 
       height_z = ((height - mean(height)) / sd(height)),
       height_z2 = scale(height))
lm_z <- lm(log_earn ~ height_z, data = earnings)
tidy(lm_z)
```

exp(0.2207) - 1
Individuals that are 1 SD higher in height make about 25% more money money on average.

# Multicollinearity

```{r}
body <- read_csv("https://dcgerard.github.io/stat_415_615/data/body.csv")
```

```{r, message = FALSE}
library(GGally)
ggpairs(body)
```

Midarm, though only moderately correlated with thigh and moderately correlated with triceps, is very strongly correlated with both thigh and triceps.

```{r}
glance(lm(midarm ~ triceps + thigh, data = body))$r.squared
```
R^2 of midarm on triceps and thigh is 0.9904

```{r}
glance(lm(midarm ~ triceps, data = body))$r.squared
glance(lm(midarm ~ thigh, data = body))$r.squared
```



Adding variables can improve prediction. But adding very highly correlated variables does not change predictions that much.


# Quadratic Terms

```{r}
library(tidyverse)
library(broom)
muscle <- read_csv("https://dcgerard.github.io/stat_415_615/data/muscle.csv")
glimpse(muscle)
```


```{r}
ggplot(muscle, aes(x = age, y = mass)) +
  geom_point()
```


To fit a curve in R, first square the predictor variable, then include it in a linear model fit.

```{r}
muscle <- mutate(muscle, age2 = age^2)
lm_musc <- lm(mass ~ age + age2, data = muscle)
tidy(lm_musc)
```

P-value of 0.081, so weak evidence of a quadratic term.

Suppose I want to make a prediction for a woman at age 70.
```{r}
newdf <- data.frame(age = 70, age2 = 70^2)
predict(lm_musc, newdata = newdf, interval = "prediction")
```


This won't work:
```{r}
lm(mass ~ age + age^2, data = muscle) |>
  tidy()
```

We can be explicit and force R to do the transformation inside lm with `I()` (asis).
```{r}
lm_musc2 <- lm(mass ~ age + I(age^2), data = muscle)
tidy(lm_musc2)
```

Lets use this fit to make a prediction at age 70.
```{r}
newdf <- data.frame(age = 70)
predict(lm_musc2, newdata = newdf, interval = "prediction")
```


You can use these formulas inside geom_smooth()

```{r}
ggplot(muscle, aes(x = age, y = mass)) +
  geom_point() +
  geom_smooth(method = "lm",
              formula = y ~ x + I(x^2))
```












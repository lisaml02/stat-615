---
title: "hw_07_mlr"
author: "lisa liubovich"
date: "2024-03-19"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r loading packages}
library(tidyverse)
library(broom)
library(GGally)
```

# Conceptual Exercises

## 1.

received help from chatgpt: <https://chat.openai.com/c/31b742a5-b995-4362-bc5f-259e08e8cfba>

### a.

Yes, it is possible to reformulate this model in terms of the general linear model by transforming $X_{i2}$ using the logarithm.

Mathematically:

We can set:

$Y_{i}$ = $\beta_{0}$ + $\beta_1$ $X_{i1}$ + $\beta_2$ log10 $X_{i2}$ + $\beta_3$ $X^2 _{i1}$ + $\varepsilon_{i}$

to $Y_{i}$ = $\beta_{0}$ + $\beta_1$ $X_{i1}$ +$\beta_2$'$X_{i2}$' +$\beta_3$ $X^2 _{i1}$ + $\varepsilon_{i}$

where $X_{i2}$' = log10 + $X_{i2}$ and $\beta_2$' = $\beta_2$

### b.

No, it is not possible to reformulate this model in terms of the general linear model. In this model, $Y_{i}$ is defined as the error term times an exponential function of a linear combination of independent variables. This is not directly a linear model because of the exponential term. There is no simple transformation that will convert this into the form of the general linear model.

### c.

Yes, it is possible to reformulate this model in terms of the general linear model by transforming $Y_{i}$ .

Mathematically:

We can transform it as:

$Y_{i}$ = log( $\beta_1$ $X_{i1}$ ) + $\beta_2$ $X_{i2}$ + $\varepsilon_{i}$

to $Y_{i}$ = log( $\beta_1$ ) + log( $X_{i1}$ ) + $\beta_2$ $X_{i2}$ + $\varepsilon_{i}$

to $Y_{i}$ = $\beta_0$ + $\beta_1$' log( $X_{i1}$ ) + $\beta_2$ $X_{i2}$ + $\varepsilon_{i}$

where $\beta_1$' = log( $\beta_1$ )

## 2.

with help from chat gpt: <https://chat.openai.com/c/31b742a5-b995-4362-bc5f-259e08e8cfba>

```{r ce 2}
# model <- lm(Y ~ X1 + I(2) + X3, data = df)
# model
```

# Brand Preference

```{r bp loading data}
brand <- read_csv("https://dcgerard.github.io/stat_415_615/data/brand.csv")
brand
```

## 1.

```{r bp 1}
ggpairs(brand)
```

This plot contains the visual representation of the relationship between like and moisture (which seems to be a positive linear relationship), like and sweetness (which appears to not be linear and have highly unequal variance), and moisture and sweetness (which also appears to not be linear and highly unequal variance).

## 2.

```{r bp 2}
lm_brand <- lm(like ~ moisture + sweetness, data = brand)
tidy(lm_brand, conf.int = TRUE)
```

Estimated regression function:

$Y_{i}$ = 37.650 + 4.425$X_{i1}$ + 4.375$X_{i2}$

where $Y_{i}$ is the degree of brand liking for observations i, $X_{i1}$ is the moisture content for observations i, and $X_{i2}$ is the sweetness content for observations i.

## 3.

coefficient for moisture: 4.425 (95% CI: 3.774, 5.076)

Interpretation:

For every additional point of brand liking, the moisture content is 4.425 units higher on average (95% CI of 3.774 units higher to 5.076 units higher), while for sweetness.

coefficient for sweetness: 4.375 (95% CI: 2.920, 5.830)

Interpretation:

For every additional point of brand liking, the sweetness content is 4.375 units higher on average (95% CI of 2.920 units higher to 5.830 units higher), adjusting for moisture.

## 4.

```{r bp 4 fitted vs residuals}
alm_brand <- augment(lm_brand)
ggplot(alm_brand, aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0)
```

Seems to be curved and pretty unequal variance.

```{r pb 4 residuals vs moisture}
ggplot(alm_brand, aes(x = moisture, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0)
```

Super curved, super unequal variance.

```{r pb 4 residuals vs sweetness}
ggplot(alm_brand, aes(x = sweetness, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0)
```

Relatively flat line, not super crazy differences in variance.

## 5.

```{r bp 5}
newdf <- data.frame(sweetness = 3, moisture = 7)
predict(object = lm_brand, newdata = newdf, interval = "confidence")
```

Predicted liking score range: between 80.296 and 83.205 points.

# Indicator Variables and Matrix Formulation

```{r ivmf reading data}
marry <- tribble(~happy, ~married,
73, "single",
79, "single",
72, "single",
58, "married",
72, "married",
74, "married",
77, "divorced",
51, "divorced",
63, "divorced")
marry
```

## 1.

Indicator variables:

$X_{i1}$ is the predictor for being married (1 if married, 0 otherwise)

$X_{i2}$ is the predictor for being divorced (1 if divorced, 0 otherwise)

that is,

married: $X_{i1}$ = 1, $X_{i2}$ = 0, divorced : $X_{i1}$ = 0, $X_{i2}$ = 1, single: $X_{i1}$ = 0, $X_{i2}$ = 0.

The linear model can be written as:

$Y_{i}$ = $\beta_0$ + $\beta_1$ $X_{1}$ + $\beta_2$ $X_{2}$ + $\varepsilon_{i}$ , where:

$\beta_0$ is the intercept term representing single individuals ( $X_{1}$ = 0, $X_{2}$ = 0)

$\beta_1$ is the coefficient representing the difference in happiness between married and single individuals

$\beta_2$ is the coefficient representing the difference in happiness between divorced and single individuals

$\varepsilon_{i}$ represents the error term with mean 0, constant variance, and uncorrelated.

## 2.

single person indicator: $X_{1}$ = 0, $X_{2}$ = 0

the model is:

$Y_{i}$ = $\beta_0$ + $\beta_1$ (0) + $\beta_2$ (0) + $\varepsilon_{i}$ --\> $Y_{i}$ = $\beta_0$ + $\varepsilon_{i}$

where $\beta_0$ is the average level of happiness for single people.

## 3.

with help from chat gpt for LaTeX: <https://chat.openai.com/c/31b742a5-b995-4362-bc5f-259e08e8cfba>

$$
X = \begin{bmatrix}
1 & X_{11} & X_{12} \\
1 & X_{21} & X_{22} \\
1 & X_{31} & X_{32} \\
1 & X_{41} & X_{42} \\
1 & X_{51} & X_{52} \\
1 & X_{61} & X_{62} \\
1 & X_{71} & X_{72} \\
1 & X_{81} & X_{82} \\
1 & X_{91} & X_{92} \\
\end{bmatrix}
$$

## 4.

with help from chatgpt for syntax: <https://chat.openai.com/c/31b742a5-b995-4362-bc5f-259e08e8cfba>

```{r  ivmf 4}
marry1 <- marry %>% 
  mutate(married_ind = ifelse(married == "married", 1, 0),
         divorced_ind = ifelse(married == "divorced", 1, 0))
lm_marry <- lm(happy ~ married_ind + divorced_ind, data = marry1)
tidy(lm_marry, conf.int = TRUE)
```

Model for divorced: $Y_{i}$ = $\beta_0$ + $\beta_1$ ( $X_{i1}$ = 0) + $\beta_2$ ( $X_{i2}$ = 1 ) + $\varepsilon_{i}$ --\> $Y_{i}$ = $\beta_0$ + $\beta_2$ + $\varepsilon_{i}$, where $\beta_2$ is the average difference in happiness between divorced and single individuals.

On average, divorced people have a happiness level that is 11 points lower than single individuals.

---
title: "class_2_7_inference"
author: "lisa liubovich"
date: "2024-02-11"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Testing H0:β1=0

## Finding a Test Statistic

We first fit the linear model using `lm()` as before, saving the output to a variable.

```{r}
library(tidyverse)
library(broom)
hibbs <- read_csv("https://dcgerard.github.io/stat_415_615/data/hibbs.csv")
lmhibbs <- lm(vote ~ growth, data = hibbs)
```

We then again use the `tidy()` function from the `{broom}` package

```{r}
t_hibbs <- tidy(lmhibbs)
```

The `statistic` variable contains the t-statistics, while the `p.value` variable contains the p-value for the test that the parameter equals 0.

```{r}
t_hibbs
```

-   In the above output the t-statistic against the null of β1=0 is 4.4.

-   So in the above output, the p-value against the null of β1=0 is `6.1e-04` = 6.1×10−4=0.000616.1×10−4=0.00061.

We can verify that the p-value can be derived directly from the t-statistic.

```{r}
n <- nrow(hibbs)
2 * pt(q = -abs(4.396), df = n - 2)
```

### exercise:

Fill in the missing values (the `NA`'s) from the following R output:

```         
## # A tibble: 2 × 5
##   term        estimate std.error statistic   p.value
##   <chr>          <dbl>     <dbl>     <dbl>     <dbl>
## 1 (Intercept)    -7.52     NA        -1.37 0.180    
## 2 drat            7.68      1.51     NA    0.0000178
```

```         
## # A tibble: 2 × 5
##   term        estimate std.error statistic   p.value
##   <chr>          <dbl>     <dbl>     <dbl>     <dbl>
## 1 (Intercept)    -7.52     5.477        -1.37 0.180    
## 2 drat            7.68      1.51     5.09    0.0000178
```

### exercise:

Verify your answer to the previous exercise by fitting a regression of `mpg` on `drat` from the `mtcars` dataset.

```{r}
data(mtcars)

lm_cars <- lm(mpg ~ drat, mtcars)
summary(lm_cars)
```

# Confidence interval for β1

```{r}
alpha <- 0.05
n <- 10
qt(alpha / 2, n - 2)
```

```{r}
qt(1 - alpha / 2, n - 2)
```

First, use `lm()` to fit the linear model.

```{r}
hibbs <- read_csv("https://dcgerard.github.io/stat_415_615/data/hibbs.csv")
lmhibbs <- lm(vote ~ growth, data = hibbs)
```

Use the `tidy()` function from the `{broom}` package, but use the `conf.int = TRUE` argument. The confidence interval will be in the `conf.low` and `conf.high` variables.

```{r}
t_hibbs <- tidy(lmhibbs, conf.int = TRUE)
select(t_hibbs, term, estimate, p.value, conf.low, conf.high)
```

You can change the level using the `conf.level` argument. E.g. to calculate a 99% confidence interval you would do

```{r}
t_hibbs <- tidy(lmhibbs, conf.int = TRUE, conf.level = 0.99)
select(t_hibbs, term, estimate, p.value, conf.low, conf.high)
```

A full interpretation might go something like this:

> We estimate that incumbants to have on average 3.1% higher vote-share in years which experience 1% more growth (95% confidence interval of 1.6% higher to 4.6% higher). We have strong evidence that this association is not due to chance alone (p=0.00061, n=16).

### exercise:

Make a plot of `qt(1 - alpha, 10)` from `alpha = 0.01` to `alpha = 0.1`. What does the the plot tell you about the width of confidence intervals as `alpha` increases?

```{r}
library(ggplot2)

# Generate values for alpha from 0.01 to 0.1
alpha_values <- seq(0.01, 0.1, length.out = 100)

# Degrees of freedom
df <- 10

# Calculate qt(1 - alpha, df) for each alpha value
t_values <- qt(1 - alpha_values, df)

# Create a data frame
data <- data.frame(alpha = alpha_values, t_value = t_values)

# Plot
ggplot(data, aes(x = alpha, y = t_value)) +
  geom_line(color = "blue", size = 1) +
  labs(title = "qt(1 - alpha, 10) vs. alpha",
       x = "Alpha",
       y = "qt(1 - alpha, 10)") +
  theme_minimal()

```

-   As alpha increases (i.e., we move from left to right along the x-axis), the values of qt(1 - alpha, 10) decrease.

-   This means that the confidence intervals become wider as alpha increases.

-   In statistical terms, a higher alpha corresponds to a lower level of confidence, which requires wider confidence intervals to account for increased uncertainty.

This behavior is consistent with the nature of confidence intervals: as we become less stringent in our requirement for statistical significance (higher alpha), we allow for more variability in the estimates, leading to wider intervals.

### exercise:

From the `mtcars` dataset, obtain the confidence interval for the slope parameter from a regression of `mpg` on `drat`.

```{r}
lm_cars <- lm(mpg ~ drat, mtcars)
t_cars <- tidy(lm_cars, conf.int = TRUE)
select(t_cars, term, estimate, p.value, conf.low, conf.high)
```

# Other interval estimates

-   There are three other types of intervals that we are interested in:

1.  A confidence interval for the mean response at a given predictor level E[Yi\|Xi].

2.  A prediction interval for a single observation at a given predictor level.

3.  A confidence band to capture the entire regression line.

## Confidence Interval for E[Yi\|Xi]

Take the output of `lm()`

```{r}
lmhibbs <- lm(vote ~ growth, data = hibbs)
```

Create a new data frame that contains the desired level(s) of Xi.

```{r}
newdf <- data.frame(growth = c(1.2, 2.4))
```

Use `predict()` with the `interval = "confidence"` argument to obtain confidence intervals. They are in the `lwr` and `upr` columns.

```{r}
predict(object = lmhibbs, newdata = newdf, interval = "confidence") %>%
  cbind(newdf)
```

Change the level using the `level` argument.

```{r}
predict(object = lmhibbs, newdata = newdf, interval = "confidence", level = 0.99) %>%
  cbind(newdf)
```

## Prediction Interval for Ŷ i(new) given Xi(new)

The steps are the exact same as the confidence intervals for the mean, except we use the `interval = "prediction"` argument in `predict()`

```{r}
lmhibbs <- lm(vote ~ growth, data = hibbs)
newdf <- data.frame(growth = c(1.2, 2.4))
predict(object = lmhibbs, newdata = newdf, interval = "prediction") %>%
  cbind(newdf)
```

Change the level using the `level` argument.

```{r}
predict(object = lmhibbs, newdata = newdf, interval = "prediction", level = 0.99) %>%
  cbind(newdf)
```

## Confidence Bands

There is no base R function that I can find that does this, but here is one if you want to use it:

```{r}
#' Working-Hotelling bands for simple linear regression.
#'
#' Intervals of the form "fit +/- w * standard-error", where w^2 is 
#' found by \code{p * qf(level, p, n - p)}.
#'
#' @param object An object of class "lm".
#' @param newdata A data frame containing the new data.
#' @param level The confidence level of the band.
#'
#' @author David Gerard
whbands <- function(object, newdata, level = 0.95) {
  stopifnot(inherits(object, "lm"))
  stopifnot(inherits(newdata, "data.frame"))
  stopifnot(is.numeric(level), length(level) == 1)
  pout <- stats::predict(object = object, 
                         newdata = newdata, 
                         se.fit = TRUE, 
                         interval = "none")
  n <- nrow(stats::model.matrix(object))
  p <- ncol(stats::model.matrix(object))
  w <- sqrt(p * stats::qf(p = level, df1 = p, df2 = n - p))
  lwr <- pout$fit - w * pout$se.fit
  upr <- pout$fit + w * pout$se.fit
  pout$fit <- cbind(fit = pout$fit, lwr = lwr, upr = upr)
  return(pout)
}
```

Applying this function to the Hibbs data

```{r}
lmhibbs <- lm(vote ~ growth, data = hibbs)
newdf <- data.frame(growth = seq(from = min(hibbs$growth), 
                                 to = max(hibbs$growth), 
                                 length.out = 100))
whfit <- whbands(object = lmhibbs, newdata = newdf)
whfit$fit %>%
  cbind(newdf) ->
  newdf

ggplot() +
  geom_point(data = hibbs, mapping = aes(x = growth, y = vote)) +
  geom_line(data = newdf, mapping = aes(x = growth, y = lwr)) +
  geom_line(data = newdf, mapping = aes(x = growth, y = upr))
```

-   "wh" stands for "Working-Hotelling".

# ANOVA approach to hypothesis testing

if HA were true, then we would expect F∗ to be large, because this would increase the size of SSE(R)−SSE(F). So we *only* compare F∗ to the right tail of F(1,n−2)distribution. Let's verify this result manually:

```{r}
lmhibbs <- lm(vote ~ growth, data = hibbs)
aout <- augment(lmhibbs)
sse_full <- sum(aout$.resid^2)
ybar <- mean(hibbs$vote)
sse_reduced <- sum((aout$vote - ybar)^2)
n <- nrow(hibbs)
df_r <- n - 1
df_f <- n - 2
f_star <- ((sse_reduced - sse_full) / (df_r - df_f)) / (sse_full / df_f)
f_star
```

```{r}
pf(q = f_star, df1 = 1, df2 = n - 2, lower.tail = FALSE)
```

this is the same as the p-value from the output of tidy().

```{r}
tidy(lmhibbs)$p.value[[2]]
```

To calculate the ANOVA table in R, use the `Anova()` function from the `{car}` package. You just pass the `lm` object to the `Anova()` function.

```{r}
library(car) # stands for companion of applied regression
Anova(mod = lmhibbs)
```

### exercise:

From the output of `Anova()`, label each of the following (which can be found in the above table): dfR, dfE, SSE, SSR.

dfR --\> Df for residuals (14)

dfE -\> Df of the error term, which is the same as dfR (14)

SSE --\> Sum of squares for the residuals (198.27)

SSR --\> sum of squares for regression (also known as explained sum of squares) (273.63)

### exercise

Using the above ANOVA table, derive the MSE (the estimated variance).

MSE = SSE/dfE

SSE = 198.27

dfE = 14

MSE = 198.27/14

MSE = 14.162

```{r}
198.27/14
```

There exists an `anova()` function in R, and it produces the same results for simple linear regression, but it produces worse results for multiple linear regression.

We can use the `linearHypothesis()` function from the `{car}` package to run a general linear hypothesis.

```{r}
linearHypothesis(model = lmhibbs, "growth = 0")
```

### exercise:

From the output of `linearHypothesis()`, label each of the following (which can be found in the above table): dfR, dfE, dfTO, SSE, SSR, SSTO.

dfR: degrees of freedom for the restricted model (model 1) (14)

dfE: degrees of freedom for the error or residuals in the unrestricted model (model 2) --\> diff in df between model 1 and 2 which is 1

dfTO: total degrees of freedom, which is the sum of degrees of freedom for the restricted model and the degrees of freedom for the error in the unrestricted model. (15 dfR + 1 dfE = 16)

SSE: Sum of squares or residuals for the unrestricted model (model 2) (198.27)

SSR: Sum of squared regression or explained variation due to the addition of the predictor variable (growth) in the unrestricted model (model 2) (273.63)

SSTO: total sum of squares, which represents the total variation in the response variable (vote). Sum of squared errors in the restricted model (model 1) (471.90)

You can use `linearHypothesis()` to test for other values of β1. E.g. H0:β1=2 versus HA:β1≠2

```{r}
linearHypothesis(model = lmhibbs, hypothesis.matrix = "growth = 2")
```

# Coefficient of Determination

You obtain the R^2^ from the `glance()` function from the `{broom}` package.

```{r}
glance(lmhibbs)
```

Interpretation: 57.98% of the variability in incumbent vote-share was explained by economic growth
